Figure 1: A diagram showing the experimental design. The yellow boxes (Steps 1 and 3) signal involvement and data aggregation from
human participants, whereas the orange boxes (Steps 2, 4, 5, and 6) denote quantitative, statistical, and computational methods. The
output of the experiment is an analysis on three corpora of human- and LLM- generated responses to various scenarios.

Figure 2: (A) In blue, schematic interface and method used in the experiment we used to generate corpus 1 (human and dv2 responses) and corpus 2 (human and dv3 responses). On the top half, the interface presented to human participants. A scenario was presented and they had to answer (yes/no) if the action taken in the scenario was "appropriate". On the bottom half, how we prompted GPT-3.5 to mimick the human experiment. (B) In orange, schematic interface used in the 3. corpus evaluation step. A  window shows the dialogue generated in the previous step. First the scenario, then the answers. The human participant had to answer three questions:(1) if they think the respondent was an AI or a human (detection) (2) If they agree with the binary (yes/no) answer (3) If they agree with the justification.

Figure 3: (A) Identified linguistic features which have been found to be different between human- and LLM-generated responses. (dv2:
text-davinci-002; dv3: text-davinci-003, dv2h: humanized dv2). dv2 is in green, dv3 in red, dv2 humanized in purple, and humans in grey. The first plot show typos frequency. The second shows the average number of words per answer. The third shows the use of first person per answer.  (B) Schematized prompting strategy to generate the humanized LLM response, by reducing size and including typos.


Figure 4: (A) Example of scenarios across three moral categories (taken from Greene et al. 2004). (B) Endorsement of the different moral actions as a function of category of scenario; ‘non moral’ (in grey) refers to scenarios with no moral stakes; ‘impersonal moral’ (in green) refers to scenarios with moral scenario whose resolution does not involve a direct, personal involvement of the participant (emotionally non-engaging); ‘personal moral’ (in pink) refers to moral scenario whose resolution involve a direct involvement of the participant (emotionally engaging). Note, what is asked in moral scenario is judging the appropriateness of the utilitarian response. There are 30 human participants, each datapoint representing a participant. The white dot is the mean. The error bars are standard error of the mean.(C) Same as (B), but for the two considered LLMs; DV2= text-davinci-002, DV3: text-davinci-003. There are 30 datapoints model, averaged over the number of agents.


Figure 5: (A) Probability of correctly detecting the source of the judgement (p(correct identification)), as a function of the scenario type in Corpus 1 (leftmost column; DV2), Corpus 2 (central column; DV3), and on average (rightmost column; Averaged). Non-moral dilemmas are in grey, impersonal moral dilemmas in green, personal moral dilemmas in pink. Each datapoint represents a participant. The white dot is the mean. The error bars are standard error of the mean.(B) Difference in agreement between the
trials featuring human-generated items and those featuring LLM-generated items as a function of the scenario type. (C) Difference in
agreement between the trials the participant declared as being human-generated and those declared to be LLM-generated (belief).

Figure 6: (A) Correct response in the detection task rate across corpus 1: dv2 (green); corpus 2: dv3 (red); corpus 3: dv2 humanized (purple). Each datapoint represents an average for a given participant. The white dot is the mean. The error bars are standard error of the mean. (B) Actual source-oriented agreement differential. (C) Declared source-oriented (belief) agreement differential.

Figure 7: (A) Probability of choosing ‘human’ in the detection task (for each corpus, i.e., dv2, dv3, dv2 humanized) as a function of different linguistic features. Items are split as a function of their length (leftmost column), the presence or not of typos (central column), and the utilization of first-person marker (rightmost column). Each sample is either in green or in orange. E.g., samples with typos are in orange, while samples without typos are in green. Each datapoint represents an average for a given participant. The white dot is the mean. The error bars are standard error of the mean.  (B) Probability of agreeing with a justification as a function of linguistic features (for each corpus, i.e., dv2, dv3, dv2 humanize).

Figure 8: SHAP beeswarm plot summarizing the impact of key features on the random forest classifier predictions. The model predicted three variables (rows: source, belief, agreement) for each experiment (columns; dv2, dv3, dv2 humanized). Each point represents a data instance, with the x-axis showing the SHAP value, which reflects the importance of a word in influencing the prediction. Thecolor represents the feature value, which in this case is the word’s frequency. Black dots indicate a low frequency of the word for a given sample of moral judgements, while colored dots signify a higher frequency. Thus, the feature value (color) goes from black (low word occurrence) to brighter colors for words that appear more frequently. Features are ordered by average absolute SHAP value,highlighting their relative importance. Features with higher SHAP values have a larger influence on the model’s output, with themost important features appearing at the top. We only display the 10 most important features.

Table 1: This table contains statistical values from two-tailed t-tests; each section corresponds to a section in the results. We report the following statistics for each t-test: Student’s t-value ($T(df)$), the significance of the p-value (***: $p < 0.001$, **: $p < 0.01$, *: $p < 0.05$, n.s.: not significant), Cohen’s d ($d$), and Bayesian factor ($BF_{10}$). The source value, when numeric, refers to the corresponding corpora.

Table 2: Performance table of 4 transformer models to predict outcomes based on the justification free text. 1. and 2. predicts the provenance of the justification text. 3. predicts whether the human rater agreed yes/no with the decision. 4. predicts whether the human correctly identified the explanation as human- or AI- generated.